# -*- coding: utf-8 -*-
"""ZERO Systems_v0

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nvL6bFs_h3chan_sx3VRahmweIBBXWWc
"""

!pip install transformers

from google.colab import files
import pandas as pd
import numpy as np
import torch

from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity

import matplotlib.pyplot as plt

"""# File Uploads

**Target Table**
"""

print("Please upload target file as CSV: \n")

# Prompt the user to upload a file
template_file_upload = files.upload()

# Get the uploaded file name
target_file_name = next(iter(template_file_upload))

# Print the uploaded file name
print("You uploaded file:", target_file_name)

target_file_df = pd.read_csv(target_file_name)

"""**Table A**"""

print("Please upload source data file as CSV: \n")

# Prompt the user to upload a file
source_data_file_upload = files.upload()

# Get the uploaded file name
source_data_file_name = next(iter(source_data_file_upload))

# Print the uploaded file name
print("You uploaded file:", source_data_file_upload)

source_file_1_df = pd.read_csv(source_data_file_name)

"""**Table B**"""

print("Please upload source data file as CSV: \n")

# Prompt the user to upload a file
source_data_file_2_upload = files.upload()

# Get the uploaded file name
source_data_file_2_name = next(iter(source_data_file_upload))

# Print the uploaded file name
print("You uploaded file:", source_data_file_2_upload)

source_file_2_df = pd.read_csv(source_data_file_2_name)

"""# LLM"""

# Load the Template table
template = pd.read_csv("template.csv")

# Load table A
table_a = pd.read_csv("table_A.csv")

# Load table B
table_b = pd.read_csv("table_B.csv")

def find_most_similar_columns(df1, df2, model_name, similarity_threshold=0.8):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name)
    
    # Create dictionaries to store the most similar columns
    most_similar_columns = {}
    decision_basis = {}

    for col1 in df1.columns:
        max_similarity = 0
        most_similar_column = None
        basis = None

        for col2 in df2.columns:
            # Concatenate all values in the column into a single string
            text1 = ' '.join(df1[col1].astype(str))
            text2 = ' '.join(df2[col2].astype(str))

            # Encode the concatenated text
            encoded_text1 = tokenizer.encode(text1, padding=True, truncation=True, return_tensors='pt')
            encoded_text2 = tokenizer.encode(text2, padding=True, truncation=True, return_tensors='pt')

            # Compute the cosine similarity between the encoded texts
            similarity = cosine_similarity(model(encoded_text1).pooler_output.detach().numpy(),
                                           model(encoded_text2).pooler_output.detach().numpy())

            # Update the most similar column if the similarity is higher
            if similarity > max_similarity:
                max_similarity = similarity
                most_similar_column = col2
                basis = {
                    'Similarity': similarity,
                    'Formats': {
                        'df1': df1[col1].dtype,
                        'df2': df2[col2].dtype
                    },
                    'Distributions': {
                        'df1': df1[col1].value_counts(normalize=True).to_dict(),
                        'df2': df2[col2].value_counts(normalize=True).to_dict()
                    },
                    # Add any other relevant features or basis for decision
                }

        # If the similarity is below the threshold, prompt the user to choose a column
        if max_similarity < similarity_threshold:
            print(f"Ambiguous similarity for column '{col1}'.")
            print(f"Similarity value: {max_similarity}")
            print(f"Choose from the following columns in df2: {df2.columns}")
            selected_column = input(f"Enter the chosen column for '{col1}': ")
            most_similar_column = selected_column
            basis = {
                'Similarity': similarity,
                'Formats': {
                    'df1': df1[col1].dtype,
                    'df2': df2[col2].dtype
                },
                'Distributions': {
                    'df1': df1[col1].value_counts(normalize=True).to_dict(),
                    'df2': df2[col2].value_counts(normalize=True).to_dict()
                },
                # Add any other relevant features or basis for decision
            }
        most_similar_columns[col1] = most_similar_column
        decision_basis[col1] = basis

    return most_similar_columns, decision_basis

model_name = 'bert-base-uncased'
similarity_threshold = 0.99

similar_columns, decision_basis = find_most_similar_columns(template, table_a, model_name, similarity_threshold)
similar_columns_b, decision_basis_b = find_most_similar_columns(template, table_b, model_name, similarity_threshold)

print(similar_columns)
print(decision_basis)

print(similar_columns_b)
print(decision_basis_b)

# Extract the similarity values from the 'decision_basis' dictionary
similarities = [basis['Similarity'] for basis in decision_basis.values()]
similarities = [arr.reshape(-1) for arr in similarities]
similarities_df = pd.DataFrame(similarities)
df_flat = similarities_df.stack().reset_index(drop=True)

# Create a bar chart of the similarity values
plt.bar(decision_basis.keys(), df_flat)
plt.xlabel('Column Name')
plt.ylabel('Similarity')
plt.title('Similarity Values')
plt.xticks(rotation=90)
plt.show()

"""From the histogram above, we can see that the 'PolicyNumber' and 'Premium' columns from table A had lower similarities compared to the 'Date', 'EmployeeName', & 'Plan' columns. Consequently, the algorithm prompted the user to choose the most similar column.

"""

# Extract the similarity values from the 'decision_basis' dictionary
similarities_b = [basis['Similarity'] for basis in decision_basis_b.values()]
similarities_b = [arr.reshape(-1) for arr in similarities_b]
similarities_b_df = pd.DataFrame(similarities_b)
df_b_flat = similarities_b_df.stack().reset_index(drop=True)

# Create a bar chart of the similarity values
plt.bar(decision_basis_b.keys(), df_b_flat)
plt.xlabel('Column Name')
plt.ylabel('Similarity')
plt.title('Similarity Values')
plt.xticks(rotation=90)
plt.show()

"""From the histogram above, we can see that the 'Date' column from table B had a lower similarity compared with columns from the Template table, compared to the 'EmployeeName', 'Plan', 'PolicyNumber', & 'Premium' columns. Consequently, the algorithm prompted the user to choose the most similar column.

# Transformation Logic

**Since such operations can be repeated quite often, and a person will edit the transformation logic, it is desirable to save this data and have the ability to retrain on it. Propose an approach for retraining and try to implement such retraining on synthetic examples (you can come up with them using GPT =))**

The function below is reponsible for mapping the value formats from tables A and B to the value format in the template table. Anyone who wishes to edit the transformation logic can do so within the context of this function, passing in the two input tables as dataframes, as well as a dictionary mapping of the best columns from the corresponding input table to the target table.

The ability to retrain the model can be accomplished by, once again, specifying the input tables as dataframes, specifying the model name, and calling the find_most_similar_columns() function from the section above.
"""

def convert_dataframe_values(df_a, df_b, column_mapping):
    new_df = pd.DataFrame()

    for col_a, col_b in column_mapping.items():
        # Get the data type of the column in dataframe A
        data_type = df_a[col_a].dtype

        def is_date(column):
          try:
              pd.to_datetime(column)
              return True
          except ValueError:
              return False

        if is_date(df_b[col_b]):
          df_b[col_b] = pd.to_datetime(df_b[col_b])

        # Convert the column values in dataframe B to the same data type
        converted_values = df_b[col_b].astype(data_type)

        # If the data type is datetime, infer the datetime format from the column value
        if data_type == 'datetime64[ns]':
            converted_values = pd.to_datetime(converted_values, infer_datetime_format=True)

        # Add the converted values to the new dataframe
        new_df[col_a] = converted_values

    return new_df

new_df = convert_dataframe_values(template, table_a, similar_columns)
print(new_df)

new_df_b = convert_dataframe_values(template, table_b, similar_columns_b)
print(new_df_b)

"""# Edge Case Discussion

**Your thoughts on edge cases and how they can be overcome:**

One edge case to consider is column dependencies. Suppose a column exists (i.e. Column Z), such that it's a combination (e.g. a grouping, concatenation, or aggregation) of other columns (i.e Column X & Column Y). In other words, the formulation of Column Z is dependent upon columns X & Y. Let's also suppose that there exists another column (Column A) that is similar to Column X, such that the column matching algorithm finds Column A to be more similar to a column in the target table than Column X. In such a scenario, we would run into an issue, since Column Z's integrity would no longer be maintained.

Another edge case that was touched upon with the datetime format deals with different value formats. Tables A and B may have different value formats compared to the template. For example, dates may be represented in different formats (as seen from above), numeric values may have different decimal places, or text values may have different capitalization, currency symbols, or units of measurement. As a pre-processing step, normalizing the value formats to convert values in tables A and B to match the format specified in the template, prior to performing any post-processing or model inference, could prove beneficial.
"""